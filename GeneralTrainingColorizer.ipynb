{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-22T20:27:14.640045Z",
     "start_time": "2025-07-22T20:27:14.572503Z"
    }
   },
   "source": "!git clone https://github.com/jantic/DeOldify.git",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'DeOldify' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cd DeOldify",
   "id": "496552dd7cd53beb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -r requirements-colab.txt",
   "id": "472e28d0b420e06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:27:18.114683Z",
     "start_time": "2025-07-22T20:27:16.415698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#NOTE:  This must be the first call in order to work properly!\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "#choices:  CPU, GPU0...GPU7\n",
    "device.set(device=DeviceId.GPU0)"
   ],
   "id": "862347f786fa7dfa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DeviceId.GPU0: 0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:27:21.149239Z",
     "start_time": "2025-07-22T20:27:18.131365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from fastai.callbacks.tensorboard import *\n",
    "from fastai.vision.gan import *\n",
    "from deoldify.generators import *\n",
    "from deoldify.critics import *\n",
    "from deoldify.dataset import *\n",
    "from deoldify.loss import *\n",
    "from deoldify.save import *"
   ],
   "id": "47358c6512fcd282",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 12 threads.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:27:21.286093Z",
     "start_time": "2025-07-22T20:27:21.164990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from deoldify.visualize import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*?Your .*? set is empty.*?\")"
   ],
   "id": "fd40cff0fcb2455f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\OneDrive\\Desktop\\final_masters\\DeOldify\\deoldify\\visualize.py:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if re.search('.*?\\.jpg', f):\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "general = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n",
    "medical = kagglehub.dataset_download(\"tawsifurrahman/covid19-radiography-database\")\n",
    "astronomy = kagglehub.dataset_download(\"razaimam45/spacenet-an-optimally-distributed-astronomy-data\")\n",
    "cctv = kagglehub.dataset_download(\"aryashah2k/large-scale-multicamera-detection-dataset\")\n",
    "\n",
    "train_ds = './data/custom_dataset/norm/full_train/'\n",
    "sample_train = './data/custom_dataset/norm/train/'\n",
    "test_ds = './data/custom_dataset/norm/test/'\n",
    "train_bandw = './data/custom_dataset/bandw/train/'\n",
    "\n",
    "os.makedirs(train_ds, exist_ok=True)\n",
    "os.makedirs(sample_train, exist_ok=True)\n",
    "os.makedirs(test_ds, exist_ok=True)\n",
    "os.makedirs(train_bandw, exist_ok=True)"
   ],
   "id": "4f74537f11e86a64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:27:22.915141Z",
     "start_time": "2025-07-22T20:27:22.909863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_and_copy_images(source_dir, target_dir, max_images=10000):\n",
    "    source_dir = Path(source_dir)\n",
    "    target_dir = Path(target_dir)\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Start counting from how many files already exist\n",
    "    existing_files = list(target_dir.glob('*.jpg'))\n",
    "    image_count = len(existing_files)\n",
    "\n",
    "    allowed_exts = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.dcm']\n",
    "    all_files = list(source_dir.rglob(\"*\"))\n",
    "\n",
    "    copied = 0  # how many images we copy *this time*\n",
    "\n",
    "    for f in tqdm(all_files, desc=f\"Processing {source_dir.name}\"):\n",
    "        ext = f.suffix.lower()\n",
    "        if ext not in allowed_exts:\n",
    "            continue\n",
    "\n",
    "        if copied >= max_images:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            out_path = target_dir / f\"{image_count:05d}.jpg\"\n",
    "\n",
    "            if ext == '.dcm':\n",
    "                dcm = pydicom.dcmread(f)\n",
    "                pixel_array = dcm.pixel_array.astype(np.float32)\n",
    "                pixel_array -= pixel_array.min()\n",
    "                pixel_array /= pixel_array.max()\n",
    "                pixel_array *= 255.0\n",
    "                img = pixel_array.astype(np.uint8)\n",
    "\n",
    "                if len(img.shape) == 2:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                cv2.imwrite(str(out_path), img)\n",
    "\n",
    "            elif ext in ['.tif', '.tiff']:\n",
    "                img = Image.open(f).convert(\"RGB\")\n",
    "                img.save(out_path)\n",
    "\n",
    "            else:\n",
    "                img = Image.open(f).convert(\"RGB\")\n",
    "                img.save(out_path)\n",
    "\n",
    "            image_count += 1\n",
    "            copied += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {f.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return copied\n"
   ],
   "id": "bcbc17784c6d80b8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "full_train_general = './data/custom_dataset/norm/full_train/general'\n",
    "\n",
    "print(convert_and_copy_images(general, full_train_general))\n",
    "print(convert_and_copy_images(medical, full_train_general))\n",
    "print(convert_and_copy_images(astronomy, full_train_general))\n",
    "print(convert_and_copy_images(cctv, full_train_general))"
   ],
   "id": "9e3a1038b91b2a68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:45:10.096020Z",
     "start_time": "2025-07-22T20:45:09.861244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "full_train_general = Path('./data/custom_dataset/norm/full_train/general')\n",
    "file_count = len(list(full_train_general.glob('*')))  # or '*.jpg' if you want only images\n",
    "\n",
    "print(f\"Number of files: {file_count}\")"
   ],
   "id": "441fb501b93ab0ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 32807\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:45:10.115160Z",
     "start_time": "2025-07-22T20:45:10.111094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_hr = './data/custom_dataset/norm/'\n",
    "path_lr = './data/custom_dataset/bandw/'\n",
    "pretrained_gen_path = './models/ColorizeArtistic_gen.pth'\n",
    "proj_id = 'ArtisticModel'\n",
    "\n",
    "gen_name = proj_id + '_gen'\n",
    "pre_gen_name = gen_name + '_0'\n",
    "crit_name = proj_id + '_crit'\n",
    "\n",
    "name_gen = proj_id + '_image_gen'\n",
    "path_gen = Path('./data/custom_dataset/name_gen')\n",
    "\n",
    "TENSORBOARD_PATH = Path('data/tensorboard/' + proj_id)\n",
    "\n",
    "nf_factor = 1.5\n",
    "pct_start = 1e-8"
   ],
   "id": "2167d344f5ebb182",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:45:10.879300Z",
     "start_time": "2025-07-22T20:45:10.158509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from fastai.vision import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "def get_data(bs:int, sz:int, keep_pct:float):\n",
    "    return get_colorize_data(sz=sz, bs=bs, crappy_path=path_lr, good_path=path_hr,\n",
    "                             random_seed=None, keep_pct=keep_pct)\n",
    "\n",
    "def get_crit_data(train_files, valid_files, labels_train, labels_valid, bs, sz):\n",
    "    all_files = train_files + valid_files\n",
    "    all_labels = labels_train + labels_valid\n",
    "\n",
    "    # Build label map BEFORE splitting\n",
    "    label_map = {str(f): lbl for f, lbl in zip(all_files, all_labels)}\n",
    "\n",
    "    # Create ImageList from all files\n",
    "    item_list = ImageList(all_files)\n",
    "\n",
    "    # Split first (important!)\n",
    "    split_idx = list(range(len(train_files))), list(range(len(train_files), len(all_files)))\n",
    "    split_items = item_list.split_by_idxs(*split_idx)\n",
    "\n",
    "    # Then label using label_from_func (fastai v1 safe)\n",
    "    labeled = split_items.label_from_func(lambda x: label_map[str(x)])\n",
    "\n",
    "    # Final databunch\n",
    "    data = (labeled\n",
    "            .transform(get_transforms(max_zoom=2.), size=sz)\n",
    "            .databunch(bs=bs)\n",
    "            .normalize(imagenet_stats))\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_preds(dl):\n",
    "    i=0\n",
    "    names = dl.dataset.items\n",
    "\n",
    "    for b in dl:\n",
    "        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\n",
    "        for o in preds:\n",
    "            o.save(path_gen/names[i].name)\n",
    "            i += 1\n",
    "\n",
    "def save_gen_images():\n",
    "    if path_gen.exists(): shutil.rmtree(path_gen)\n",
    "    path_gen.mkdir(exist_ok=True)\n",
    "    data_gen = get_data(bs=bs, sz=sz, keep_pct=0.085)\n",
    "    save_preds(data_gen.fix_dl)\n",
    "    PIL.Image.open(path_gen.ls()[0])"
   ],
   "id": "44a48a6679cb245f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:45:10.967648Z",
     "start_time": "2025-07-22T20:45:10.893244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "def save_cv2(filename, image, subset):\n",
    "    output_dir = Path(path_lr) / subset\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dest = output_dir / filename\n",
    "    cv2.imwrite(str(dest), image)\n",
    "\n",
    "def save_cv2_before(filename, image, subset):\n",
    "    output_dir = Path(path_hr) / subset\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dest = output_dir / filename\n",
    "    cv2.imwrite(str(dest), image)\n",
    "\n",
    "def add_white_gaussian_noise(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    std = random.uniform(5, 40)  # camera ISO variability\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_white_gaussian_noise.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    noise = np.random.normal(0, std, image.shape).astype(np.float32)\n",
    "    noisy = np.clip(image.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "    save_cv2(filename, noisy, subset)\n",
    "\n",
    "def apply_gaussian_blur(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    ksize = random.choice([3, 5, 7])\n",
    "    sigma = random.uniform(0.5, 2.5)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_gaussian_blur.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    blurred = cv2.GaussianBlur(image, (ksize, ksize), sigmaX=sigma)\n",
    "    save_cv2(filename, blurred, subset)\n",
    "\n",
    "def jpeg_compression(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    quality = random.randint(5, 50)  # simulate variable lossy compression\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_jpeg_compression.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
    "    _, encimg = cv2.imencode('.jpg', image, encode_param)\n",
    "    compressed = cv2.imdecode(encimg, 1)\n",
    "    save_cv2(filename, compressed, subset)\n",
    "\n",
    "\n",
    "def chromatic_aberration(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    shift = random.randint(1, 4)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_chromatic_aberration.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    b, g, r = cv2.split(image)\n",
    "    r = np.roll(r, shift, axis=1)\n",
    "    b = np.roll(b, -shift, axis=1)\n",
    "    aberrated = cv2.merge((b, g, r))\n",
    "    save_cv2(filename, aberrated, subset)\n",
    "\n",
    "def photometric_distortion(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    gamma = random.uniform(0.6, 1.4)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_photometric.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    adjusted = exposure.adjust_gamma(image, gamma=gamma)\n",
    "    adjusted = (adjusted * 255).astype(np.uint8)\n",
    "    save_cv2(filename, adjusted, subset)\n",
    "\n",
    "def structural_artifacts(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    spacing = random.randint(20, 50)\n",
    "    thickness = random.randint(1, 3)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_structural_artifacts.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    artifact = image.copy()\n",
    "    h, w = artifact.shape[:2]\n",
    "    for i in range(0, h, spacing):\n",
    "        cv2.line(artifact, (0, i), (w, i), (255, 255, 255), thickness)\n",
    "    save_cv2(filename, artifact, subset)\n",
    "\n",
    "def apply_barrel_distortion(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_barrel_distortion.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    h, w = image.shape[:2]\n",
    "    k = random.uniform(0.00002, 0.0001)\n",
    "    fx, fy = w, h\n",
    "    cx, cy = w / 2, h / 2\n",
    "    K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "    D = np.array([-k, 0, 0, 0])\n",
    "    map1, map2 = cv2.initUndistortRectifyMap(K, D, None, K, (w, h), cv2.CV_32FC1)\n",
    "    distorted = cv2.remap(image, map1, map2, interpolation=cv2.INTER_LINEAR)\n",
    "    save_cv2(filename, distorted, subset)\n",
    "\n",
    "def apply_pincushion_distortion(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_pincushion_distortion.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    h, w = image.shape[:2]\n",
    "    k = random.uniform(0.00002, 0.0001)\n",
    "    fx, fy = w, h\n",
    "    cx, cy = w / 2, h / 2\n",
    "    K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "    D = np.array([k, 0, 0, 0])\n",
    "    map1, map2 = cv2.initUndistortRectifyMap(K, D, None, K, (w, h), cv2.CV_32FC1)\n",
    "    distorted = cv2.remap(image, map1, map2, interpolation=cv2.INTER_LINEAR)\n",
    "    save_cv2(filename, distorted, subset)\n",
    "\n",
    "def apply_mustache_distortion(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_mustache_distortion.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    h, w = image.shape[:2]\n",
    "    k1 = random.uniform(-0.0001, -0.00002)\n",
    "    k2 = random.uniform(0.00002, 0.0001)\n",
    "    fx, fy = w, h\n",
    "    cx, cy = w / 2, h / 2\n",
    "    K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "    D = np.array([k1, k2, 0, 0])\n",
    "    map1, map2 = cv2.initUndistortRectifyMap(K, D, None, K, (w, h), cv2.CV_32FC1)\n",
    "    distorted = cv2.remap(image, map1, map2, interpolation=cv2.INTER_LINEAR)\n",
    "    save_cv2(filename, distorted, subset)\n",
    "\n",
    "# 1. Intensity Non-Uniformity (e.g., scanner illumination variation)\n",
    "def intensity_non_uniformity(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_intensity_nonuniformity.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    rows, cols = image.shape[:2]\n",
    "    min_factor = random.uniform(0.4, 0.7)\n",
    "    max_factor = random.uniform(1.2, 1.6)\n",
    "    gradient = np.tile(np.linspace(min_factor, max_factor, cols), (rows, 1)).astype(np.float32)\n",
    "    non_uniform = image.astype(np.float32)\n",
    "    for i in range(3):\n",
    "        non_uniform[:, :, i] *= gradient\n",
    "    non_uniform = np.clip(non_uniform, 0, 255).astype(np.uint8)\n",
    "    save_cv2(filename, non_uniform, subset)\n",
    "\n",
    "# 2. Motion Artifacts (e.g., subject or sensor movement)\n",
    "def motion_artifacts(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_motion_artifacts.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    kernel_size = random.choice([7, 11, 15, 21])\n",
    "    kernel_motion_blur = np.zeros((kernel_size, kernel_size))\n",
    "    kernel_motion_blur[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)\n",
    "    kernel_motion_blur = kernel_motion_blur / kernel_size\n",
    "    motion_blur = cv2.filter2D(image, -1, kernel_motion_blur)\n",
    "    save_cv2(filename, motion_blur, subset)\n",
    "\n",
    "# 3. Partial Volume Effects (e.g., pixel mixing at boundaries)\n",
    "def partial_volume_effects(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_partial_volume.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    scale = random.choice([1.5, 2.0, 2.5])\n",
    "    down = cv2.resize(image, (int(image.shape[1] // scale), int(image.shape[0] // scale)))\n",
    "    up = cv2.resize(down, (image.shape[1], image.shape[0]))\n",
    "    alpha = random.uniform(0.3, 0.7)\n",
    "    blended = cv2.addWeighted(image, alpha, up, 1 - alpha, 0)\n",
    "    save_cv2(filename, blended, subset)\n",
    "\n",
    "# 4. Beam Hardening (e.g., cupping effect in CT)\n",
    "def beam_hardening(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_beam_hardening.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    rows, cols = image.shape[:2]\n",
    "    strength = random.uniform(0.3, 0.6)\n",
    "    x = np.linspace(-1, 1, cols)\n",
    "    y = np.linspace(-1, 1, rows)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    radius = np.sqrt(xv**2 + yv**2)\n",
    "    mask = 1 - strength * radius\n",
    "    mask = np.clip(mask, 0.5, 1.0)\n",
    "    darkened = image.astype(np.float32)\n",
    "    for i in range(3):\n",
    "        darkened[:, :, i] *= mask\n",
    "    darkened = np.clip(darkened, 0, 255).astype(np.uint8)\n",
    "    save_cv2(filename, darkened, subset)\n",
    "\n",
    "# 5. Metal Artifacts (e.g., bright streaks from implants in CT)\n",
    "def metal_artifacts(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_metal_artifacts.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    artifact = image.copy()\n",
    "    h, w = artifact.shape[:2]\n",
    "    spacing = random.randint(30, 80)\n",
    "    thickness = random.randint(1, 3)\n",
    "    for i in range(random.randint(10, 30), h, spacing):\n",
    "        cv2.line(artifact, (0, i), (w, i), (255, 255, 255), thickness)\n",
    "    for i in range(random.randint(10, 30), w, spacing):\n",
    "        cv2.line(artifact, (i, 0), (i, h), (255, 255, 255), thickness)\n",
    "    save_cv2(filename, artifact, subset)\n",
    "\n",
    "# Compression Artifacts\n",
    "def compression_artifacts(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    temp_file = f\"{image_name}_compressed_temp.jpg\"\n",
    "    filename = f\"{image_name}_compression_artifacts.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    quality = random.randint(5, 15)  # very high compression\n",
    "    cv2.imwrite(temp_file, image, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
    "    compressed = cv2.imread(temp_file)\n",
    "    save_cv2(filename, compressed, subset)\n",
    "    Path(temp_file).unlink()\n",
    "\n",
    "# Low Resolution\n",
    "def low_resolution(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_low_resolution.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    factor = random.choice([4, 6, 8])\n",
    "    small = cv2.resize(image, (image.shape[1]//factor, image.shape[0]//factor))\n",
    "    low_res = cv2.resize(small, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    save_cv2(filename, low_res, subset)\n",
    "\n",
    "# Aliasing\n",
    "def aliasing(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_aliasing.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    rows, cols = image.shape[:2]\n",
    "    freq = random.randint(2, 6)\n",
    "    mask = np.indices((rows, cols)).sum(axis=0) % freq\n",
    "    mask = cv2.resize((mask == 0).astype(np.uint8), (cols, rows), interpolation=cv2.INTER_NEAREST)\n",
    "    aliased = image.copy()\n",
    "    aliased[mask == 1] = 0\n",
    "    save_cv2(filename, aliased, subset)\n",
    "\n",
    "# Color Shift / White Balancing Error\n",
    "def color_shift_white_balance(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_color_shift.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    shift = np.array([\n",
    "        random.randint(10, 30),\n",
    "        random.randint(-20, 5),\n",
    "        random.randint(-20, 5)\n",
    "    ], dtype=np.int16)  # BGR shift\n",
    "    shifted = image.astype(np.int16) + shift\n",
    "    shifted = np.clip(shifted, 0, 255).astype(np.uint8)\n",
    "    save_cv2(filename, shifted, subset)\n",
    "\n",
    "# Rolling Shutter Distortion\n",
    "def rolling_shutter_distortion(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_rolling_shutter.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    distorted = image.copy()\n",
    "    amplitude = random.randint(5, 15)\n",
    "    frequency = random.uniform(80, 140)\n",
    "    for i in range(image.shape[0]):\n",
    "        offset = int(amplitude * np.sin(2 * np.pi * i / frequency))\n",
    "        distorted[i] = np.roll(image[i], offset, axis=0)\n",
    "    save_cv2(filename, distorted, subset)\n",
    "\n",
    "# Atmospheric Distortion (Seeing)\n",
    "def atmospheric_distortion(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_atmospheric_distortion.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    for _ in range(random.randint(2, 5)):\n",
    "        dx = np.random.normal(0, 1.5)\n",
    "        dy = np.random.normal(0, 1.5)\n",
    "        M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "        image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
    "    blurred = cv2.GaussianBlur(image, (3, 3), sigmaX=1.2)\n",
    "    save_cv2(filename, blurred, subset)\n",
    "\n",
    "\n",
    "# Cosmic Ray Artifacts (bright pixels)\n",
    "def cosmic_ray_artifacts(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_cosmic_ray.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    result = image.copy()\n",
    "    h, w = image.shape[:2]\n",
    "    for _ in range(np.random.randint(5, 16)):\n",
    "        x = np.random.randint(0, w)\n",
    "        y = np.random.randint(0, h)\n",
    "        radius = np.random.randint(1, 4)\n",
    "        color = (255, 255, 255)\n",
    "        cv2.circle(result, (x, y), radius, color, -1)\n",
    "    save_cv2(filename, result, subset)\n",
    "\n",
    "# Tracking Errors (drift or stacking misalignment)\n",
    "def tracking_errors(image_path, subset):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_name = Path(image_path).stem\n",
    "    filename = f\"{image_name}_tracking_error.jpeg\"\n",
    "    save_cv2_before(filename, image, subset)\n",
    "    dx = random.randint(2, 6)\n",
    "    dy = random.randint(2, 6)\n",
    "    shifted1 = np.roll(image, dx, axis=1)\n",
    "    shifted2 = np.roll(image, -dy, axis=0)\n",
    "    stacked = cv2.addWeighted(image, 0.5, shifted1, 0.3, 0)\n",
    "    stacked = cv2.addWeighted(stacked, 1.0, shifted2, 0.2, 0)\n",
    "    save_cv2(filename, stacked, subset)\n",
    "\n",
    "\n",
    "def create_general_training_images(fn, i, subset):\n",
    "    add_white_gaussian_noise(fn, subset)\n",
    "    apply_gaussian_blur(fn, subset)\n",
    "    jpeg_compression(fn, subset)\n",
    "    photometric_distortion(fn, subset)\n",
    "    intensity_non_uniformity(fn, subset)\n",
    "    motion_artifacts(fn, subset)\n",
    "    partial_volume_effects(fn, subset)\n",
    "    beam_hardening(fn, subset)\n",
    "    metal_artifacts(fn, subset)\n",
    "    compression_artifacts(fn, subset)\n",
    "    low_resolution(fn, subset)\n",
    "    aliasing(fn, subset)\n",
    "    color_shift_white_balance(fn, subset)\n",
    "    rolling_shutter_distortion(fn, subset)\n",
    "    atmospheric_distortion(fn, subset)\n",
    "    chromatic_aberration(fn, subset)\n",
    "    cosmic_ray_artifacts(fn, subset)\n",
    "    tracking_errors(fn, subset)\n"
   ],
   "id": "dcc37d9cbc223a50",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:45:11.059411Z",
     "start_time": "2025-07-22T20:45:10.972245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "\n",
    "full_train_general = Path(full_train_general)\n",
    "il = list(full_train_general.glob('*'))\n",
    "random.shuffle(il)\n",
    "print(len(il))"
   ],
   "id": "489f6b5348cc5efa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32807\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T22:22:23.876444Z",
     "start_time": "2025-07-22T20:45:11.084395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split into 70% train, 15% val, 15% test\n",
    "general_train_files, general_temp_files = train_test_split(il, test_size=0.3, random_state=42)\n",
    "general_valid_files, general_test_files = train_test_split(general_temp_files, test_size=0.5, random_state=42)\n",
    "\n",
    "# Launch for training set\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(create_general_training_images, str(fn), i, 'train') for i, fn in enumerate(general_train_files)]\n",
    "    for f in futures:\n",
    "        f.result()\n",
    "\n",
    "# Launch for validation set\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(create_general_training_images, str(fn), i, 'valid') for i, fn in enumerate(general_valid_files)]\n",
    "    for f in futures:\n",
    "        f.result()\n",
    "\n",
    "# Launch for test set\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(create_general_training_images, str(fn), i, 'test') for i, fn in enumerate(general_test_files)]\n",
    "    for f in futures:\n",
    "        f.result()\n"
   ],
   "id": "b89c48bc2b17264c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bs=88\n",
    "sz=64\n",
    "keep_pct=1.0\n",
    "data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)\n",
    "learn_gen = gen_learner_deep(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor)\n",
    "# state = torch.load(pretrained_gen_path, weights_only=False)\n",
    "# learn_gen.model.load_state_dict(state['model'])\n",
    "# learn_gen.callback_fns.append(partial(ImageGenTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GenPre'))\n",
    "learn_gen.fit_one_cycle(1, pct_start=0.8, max_lr=slice(1e-4, 3e-4))\n",
    "learn_gen.save(pre_gen_name)\n",
    "learn_gen.unfreeze()\n",
    "learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(1e-5, 1e-4))\n",
    "learn_gen.save(pre_gen_name)"
   ],
   "id": "7bdf78c438422696",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bs=22\n",
    "sz=128\n",
    "keep_pct=1.0\n",
    "learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)\n",
    "learn_gen.unfreeze()\n",
    "learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(1e-7,1e-4))\n",
    "learn_gen.save(pre_gen_name)"
   ],
   "id": "729716ef0479aebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:20:35.242119Z",
     "start_time": "2025-07-23T13:20:35.232708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "old_checkpoint_num = 0\n",
    "checkpoint_num = old_checkpoint_num + 1\n",
    "gen_old_checkpoint_name = gen_name + '_' + str(old_checkpoint_num)\n",
    "gen_new_checkpoint_name = gen_name + '_' + str(checkpoint_num)\n",
    "crit_old_checkpoint_name = crit_name + '_' + str(old_checkpoint_num)\n",
    "crit_new_checkpoint_name= crit_name + '_' + str(checkpoint_num)"
   ],
   "id": "3dcf403d0b639d53",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bs=11\n",
    "sz=128\n",
    "data_gen_path = Path('./data/custom_dataset/bandw/models') / f'{gen_old_checkpoint_name}.pth'\n",
    "learn_gen = gen_learner_deep(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor)\n",
    "state = torch.load(Path(data_gen_path), weights_only=False)\n",
    "learn_gen.model.load_state_dict(state['model'])\n",
    "save_gen_images()"
   ],
   "id": "1a164475c76a98ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:41:40.363318Z",
     "start_time": "2025-07-23T13:41:40.153215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "real_files = get_image_files(Path('./data/custom_dataset/norm/full_train'))\n",
    "fake_files = get_image_files(Path('./data/custom_dataset/name_gen'))\n",
    "\n",
    "real_labels = [1] * len(real_files)\n",
    "fake_labels = [0] * len(fake_files)\n",
    "\n",
    "all_files = real_files + fake_files\n",
    "all_labels = real_labels + fake_labels\n",
    "\n",
    "train_files, valid_files, labels_train, labels_valid = train_test_split(\n",
    "    all_files, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
    ")"
   ],
   "id": "646300ddc34adcf9",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if old_checkpoint_num == 0:\n",
    "    bs=64\n",
    "    sz=128\n",
    "    learn_gen=None\n",
    "    gc.collect()\n",
    "    data_crit = get_crit_data(train_files, valid_files, labels_train, labels_valid, bs, sz)\n",
    "    data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\n",
    "    learn_critic = colorize_crit_learner(data=data_crit, nf=256)\n",
    "    learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\n",
    "    learn_critic.fit_one_cycle(6, 1e-3)\n",
    "    learn_critic.save(crit_old_checkpoint_name)"
   ],
   "id": "f1289d2a0335dee7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bs=32\n",
    "sz=128\n",
    "data_crit = get_crit_data(train_files, valid_files, labels_train, labels_valid, bs, sz)\n",
    "data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\n",
    "learn_critic = colorize_crit_learner(data=data_crit, nf=256)\n",
    "# state = torch.load(f'./models/{crit_old_checkpoint_name}.pth', weights_only=False)\n",
    "# learn_critic.model.load_state_dict(state['model'])\n",
    "learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\n",
    "learn_critic.fit_one_cycle(4, 1e-4)\n",
    "learn_critic.save(crit_new_checkpoint_name)"
   ],
   "id": "c0cbcd7dab4dac2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learn_crit=None\n",
    "learn_gen=None\n",
    "gc.collect()\n",
    "lr=1e-5\n",
    "sz=128\n",
    "bs=12\n",
    "data_crit = get_crit_data(train_files, valid_files, labels_train, labels_valid, bs, sz)\n",
    "learn_crit = colorize_crit_learner(data=data_crit, nf=256)\n",
    "state = torch.load(f'./models/{crit_new_checkpoint_name}.pth', weights_only=False)\n",
    "learn_crit.model.load_state_dict(state['model'])\n",
    "\n",
    "learn_gen = gen_learner_deep(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor)\n",
    "#data/custom_dataset/bandw/models/ArtisticModel_gen_0.pth\n",
    "state = torch.load(f'./data/custom_dataset/bandw/models/{gen_old_checkpoint_name}.pth', weights_only=False)\n",
    "learn_gen.model.load_state_dict(state['model'])\n",
    "\n",
    "switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\n",
    "learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.0,2.0), show_img=False, switcher=switcher,\n",
    "                                 opt_func=partial(optim.Adam, betas=(0.,0.9)), wd=1e-3)\n",
    "learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))\n",
    "learn.callback_fns.append(partial(GANTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GanLearner', visual_iters=100))\n",
    "learn.callback_fns.append(partial(GANSaveCallback, learn_gen=learn_gen, filename=gen_new_checkpoint_name, save_iters=100))"
   ],
   "id": "5ad1f964177ae4e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learn.data = get_data(sz=sz, bs=bs, keep_pct=0.03)\n",
    "learn_gen.freeze_to(-1)\n",
    "learn.fit(1,lr)"
   ],
   "id": "ebee9a57bc2cfe36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:55:18.487360Z",
     "start_time": "2025-07-23T15:55:18.131390Z"
    }
   },
   "cell_type": "code",
   "source": "learn_gen.save(pre_gen_name)",
   "id": "12510d9de523e50c",
   "outputs": [],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
